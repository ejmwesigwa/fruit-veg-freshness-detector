{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70db034-bbd5-44ef-8207-0bf605c4c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load trained ResNet18 model\n",
    "model = torch.load('resnet18_fruit_model.pth', map_location=torch.device('cpu'))  # adjust path if needed\n",
    "model.eval()\n",
    "\n",
    "# Define transforms (same as during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define hook to capture gradients and activations\n",
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "def save_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def save_gradient(name):\n",
    "    def hook(model, grad_input, grad_output):\n",
    "        gradients[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "# Register hook to the last convolutional layer\n",
    "target_layer = model.layer4[1].conv2  # works for standard ResNet18\n",
    "target_layer.register_forward_hook(save_activation(\"features\"))\n",
    "target_layer.register_backward_hook(save_gradient(\"features\"))\n",
    "\n",
    "# Grad-CAM function\n",
    "def generate_gradcam(image_tensor, class_idx):\n",
    "    model.zero_grad()\n",
    "    output = model(image_tensor.unsqueeze(0))\n",
    "    pred_class = output.argmax().item()\n",
    "\n",
    "    score = output[0, class_idx]\n",
    "    score.backward()\n",
    "\n",
    "    grads = gradients[\"features\"]\n",
    "    acts = activations[\"features\"]\n",
    "    weights = grads.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    cam = (weights * acts).sum(dim=1).squeeze()\n",
    "    cam = torch.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    cam = cam / cam.max()\n",
    "    cam = cam.numpy()\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    return cam, pred_class\n",
    "\n",
    "# Plotting function\n",
    "def show_gradcam(image_path, class_idx, true_label):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(img)\n",
    "\n",
    "    cam, pred_class = generate_gradcam(input_tensor, class_idx)\n",
    "\n",
    "    img_np = np.array(img.resize((224, 224)))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    overlay = heatmap + np.float32(img_np) / 255\n",
    "    overlay = overlay / overlay.max()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"True: {true_label}, Predicted: {pred_class}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ðŸ‘‡ Example usage: use 3 image paths from poorly classified classes\n",
    "# Replace with actual paths from your test dataset\n",
    "examples = [\n",
    "    ('test/Bellpepper__Rotten/img123.jpg', class_idx_for_bellpepper_rotten, 'Bellpepper__Rotten'),\n",
    "    ('test/Tomato__Rotten/img456.jpg', class_idx_for_tomato_rotten, 'Tomato__Rotten'),\n",
    "    ('test/Potato__Rotten/img789.jpg', class_idx_for_pomegranate_rotten, 'Potato__Rotten'),\n",
    "]\n",
    "\n",
    "for img_path, class_idx, true_label in examples:\n",
    "    show_gradcam(img_path, class_idx, true_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
